---
title: "Assignment1"
output: html_document
Name: Eliya Tiram
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

# DATA PREPARATION

• ~~Removing duplicate or irrelevant observations [**Eliya**]~~ DONE

• ~~Fix structural errors (usually coding errors, trailing blanks in
labels, lower/upper case consistency, etc.). [**Eliya**]~~ DONE

~~• Check data types. Dates should be coded as such and factors should
have level names (if possible, levels have to be set and clarify the
variable they belong to). This point is sometimes included under data
transformation process. New derived variables are to be produced
sometimes scaling and/or normalization (range/shape changes to numeric
variables) or category regrouping for factors (nominal/ordinal).
[**Eliya**]~~ DONE

::: {style="color: red;"}
• Filter unwanted outliers. Univariate and multivariate outliers have to
be highlighted.Remove register/erase values and set NA for univariate
outiers. [**Eliya**] *--I HAVE PATIALLY DID IT, STILL NEED TO UNDERSTAND
SOMETHING THERE*
:::

::: {style="color: red;"}
• ~~Handle missing data: figure out why the data is missing. Data
imputation is to be considered when the aim is modelling (imputation has
to be validated)~~. [**Achraf**]
:::

• Data validation is mixed of 'common sense and sector knowledge': Does
the data make sense? Does the data follow the appropriate rules for its
field? Does it prove or disprove the working theory, or bring any
insight to light? Can you find trends in the data to help you form a new
theory? If not, is that because of a data quality issue? [**Achraf**]

# TASKS

-   ~~Create factors for qualitative variables. [Eliya]~~ *DONE*

-   ~~Determine if the response variable (charges) has an acceptably
    normal distribution~~ [**Achraf**]

-   Address tests to discard serial correlation. [**Eliya**] *DONE*

-   ~~Detect univariant and multivariant outliers, errors and missing
    values (if any) and apply animputation technique if needed.~~
    [Achraf]

-   Preliminary exploratory analysis to describe the relationships
    observed has to be undertaken. [**Eliya**]

-   If you can improve linear relations or limit the effect of
    influential data, you must consider the suitable transformations for
    variables. [**Achraf**]

-   Apart from the original factor variables, you can consider other
    categorical variables that can be defined from categorized numeric
    variables. [**Eliya**]

-   You must take into account possible interactions between categorical
    and numerical variables. [**Eliya**]

-   When building the model, you should study the presence of
    multicollinearity and try to reduce their impact on the model for
    easier interpretation. [**Achraf**]

-   You should build the model using a technique for selecting variables
    (removing no significant predictors and/or stepwise selection of the
    best models). [**Achraf**]

-   The validation of the model has to be done with graphs and / or
    suitable tests to verify model assumptions. [**Achraf**]

-   You must include the study of unusual and / or influential data.
    [**Achraf**]

-   The resulting model should be interpreted in terms of the
    relationships of selected predictors and its effect on the response
    variable. [**Eliya**]

# ASSIGNMENT

```{r}
library(GGally)
#install.packages("data.table")
library(data.table)
library(car)
library(rpart)
library(chemometrics)
#install.packages("mvoutlier")
library(mvoutlier)
library(sgeostat)
library(lmtest)
```

Preparing the data in the environment

```{r}
# Clear plots
if(!is.null(dev.list())) dev.off()
# Clean workspace
rm(list=ls())
#load data
df <- read.csv("insurance.csv")
```

### Data cleaning

#### Data format

```{r}
is.null(df) #no nulls in the data
replace(df,which(df %like% " "), '') #close all blank spaces
which(df=="") #no blanks found in the data
#check for distinct values and whether there are differences in them
unique(df$sex) #expecting 2 values
unique(df$smoker) #expecting 2 values
unique(df$region) #expecting 4 values
#we can see that data is consistent for categorical variables
df$f.sex <- factor(df$sex,labels = c("female","male"));
df$f.smoker <- factor(df$smoker,labels = c("no","yes"))
df$f.region <- factor(df$region,labels = c("northeast","northwest","southeast","southwest"))
summary(df) #from the summary we can see the factor values, it seems that sex and region are distributed equally and not much smokers compare to the non smokers.
dim(df)
unique(df)
#There is only one observation which repeat twice, it makes sense that a person with the same properties will have the same charge and since it's only one we decide to leave it there.
#outliers

```

#### Outlier detection

##### Univariate

We can see extreme outliers for both charges and bmi, since it's just serval observation it might be the case that for a certain bmi, age or smokers the charge value is raising by a lot compare to the rest. from looking at the high value of column charges it can be seen that all are smokers and mid-high bmi, also some of the ages I see are relatively high.
For the target variable we can see there is no lower bound for extreme and mild outliers, it's also can be seen on the Boxplot().
For variable bmi, mild outliers on the upper bound and no sever upper bound outliers and not lower bound outliers.

```{r}

par(mfrow=c(1,2))
Boxplot(df$charges)
Boxplot(df$bmi)
Boxplot(df$age)
Boxplot(df$children)
#

# treat outliers for charges variable
sevout<-quantile(df$charges,0.75,na.rm=TRUE)+3*(quantile(df$charges,0.75,na.rm=TRUE)-quantile(df$charges,0.25,na.rm=TRUE))
sevout
sev_out_lower <- quantile(df$charges,0.25,na.rm=TRUE)-3*(quantile(df$charges,0.75,na.rm=TRUE)-quantile(df$charges,0.25,na.rm=TRUE))

mist<-quantile(df$charges,0.75,na.rm=TRUE)+1.5*(quantile(df$charges,0.75,na.rm=TRUE)-quantile(df$charges,0.25,na.rm=TRUE))
mist

mist_out_lower <- quantile(df$charges,0.25,na.rm=TRUE)-1.5*(quantile(df$charges,0.75,na.rm=TRUE)-quantile(df$charges,0.25,na.rm=TRUE))

# get list of outliers
loutse<-which(df$charges>sevout);length(loutse)
loutmist <-which(df$charges>mist);length(loutmist)
low_out_sever <- which(df$charges<sev_out_lower);low_out_sever
low_out_mild <- which(df$charges<mist_out_lower);low_out_mild

table(loutse)
table(loutmist)

# see outliers
boxplot(df$charges)
abline(h=sevout,col="red")
abline(h=mist,col="yellow")

# Since there are only 6 severe outliers, the observations will be replaced for the median value

df[loutse, c(7)] <- median(df[,c(7)])

# check severe outliers for bmi atrribute
sevout_bmi<-quantile(df$bmi,0.75,na.rm=TRUE)+3*(quantile(df$bmi,0.75,na.rm=TRUE)-quantile(df$bmi,0.25,na.rm=TRUE));sevout_bmi
mist_bmi <- quantile(df$bmi,0.75,na.rm=TRUE)+1.5*(quantile(df$bmi,0.75,na.rm=TRUE)-quantile(df$bmi,0.25,na.rm=TRUE))
loutse_bmi<-which(df$bmi>sevout_bmi);length(loutse_bmi) # no severe outliers for bmi
colSums(is.na(df))
serout_lower_bmi <- quantile(df$bmi,0.25,na.rm=TRUE)-3*(quantile(df$bmi,0.75,na.rm=TRUE)-quantile(df$bmi,0.25,na.rm=TRUE));serout_lower
mist_lower_bmi <- quantile(df$bmi,0.25,na.rm=TRUE)-1.5*(quantile(df$bmi,0.75,na.rm=TRUE)-quantile(df$bmi,0.25,na.rm=TRUE));serout_lower

up_sever_bmi <- which(df$bmi > sevout_bmi); up_sever_bmi
up_mild_bmi <- which(df$bmi > mist_bmi); up_mild_bmi
low_sever_bmi <- which(df$bmi < serout_lower_bmi); low_sever_bmi
low_mild_bmi <- which(df$bmi < mist_lower_bmi); low_mild_bmi

```

##### Multivariate

When looking at the results of the multivariate outliers we see that all the highest charges are for smokers for 21 out 26 identified outliers. In addition, the rest of 5 observations has high bmi or 5 children, which is the highest value in the data. Also, we see observations number 33, 167, where the ages of women are 19 and 20 and it makes no sense to have 5 children at this age. Another observation in the outliers is the highest bmi (53.130) with a relativity low charges (1163.463 - 10th low out of all observations).

From general look at the data we see observations number: *(WE CAN ADD THIS PART IN DATA ANALIYSIS EVEN THOUGH SOME OF THESE OBSERVATIONS COMES AS OUTLIERS )*
370, 1096, 1205 age (18 with at least 3 children)
33, 1196 (age 19 with 5 and 3 children)
167,985 (age 20 with 5 children)

```{r}
res.out<-Moutlier(df[,c(7,3,1,4)],quantile=0.975)
str(res.out)
plot(df$charges,df$bmi)
res.out$cutoff

quantile(res.out$md,seq(0,1,0.025))
which((res.out$md > res.out$cutoff) & (res.out$rd > res.out$cutoff))
ddf <-df[which((res.out$md > res.out$cutoff) & (res.out$rd > res.out$cutoff)),]
summary(df)
summary(ddf)
#View(ddf)

plot( res.out$md, res.out$rd )
#text(res.out$md, res.out$rd, labels=rownames(df),adj=1, cex=0.5)
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")
#I NEED TO REMEMBER HOW TO INTERPETE THIS PLOT - I KNOW THAT WHATEVER ABOVE THE CUTOFF IS A MULTIVARIATE OUTLIER BUT SINCE MY EXPLANATION ABOVE WE ARE NOT GOING TO DELETE THEM OF IMPUTE THEM CAUSE WE HAVE EXPLNATION.
res.out$cutoff^2
qchisq(0.975,4)

#aq.plot(df[,c(3,7)],delta = qchisq(0.95,df=ncol(x)),alpha = 0.05)

```

-   Detect univariant and multivariant outliers, errors and missing
    values (if any) and apply animputation technique if needed. [Achraf]

(taking into acount all features)

```{r}
res.out<-Moutlier(df[,c(1,3,4,7)],quantile=0.999)
str(res.out)
plot(df$charges,df$bmi)
res.out$cutoff

quantile(res.out$md,seq(0,1,0.001))
which((res.out$md > res.out$cutoff) & (res.out$rd > res.out$cutoff))
df <-df[-c(which((res.out$md > res.out$cutoff) & (res.out$rd > res.out$cutoff))),]
summary(df)

plot( res.out$md, res.out$rd )
#text(res.out$md, res.out$rd, labels=rownames(df),adj=1, cex=0.5)
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")
```

#### Missing data

```{r}

# check missing data

# Imputating using median is used in the numeric variable "charges" for severe outliers
# there is no missing data in the dataframe so no further imputation is needed 
colSums(is.na(df))

```

#### Data Validation

In this section I am going to check if variables "make sense"

### Explanatory data analysis

```{r}
summary(df)
#numeric variables
summary(df[,c(1,3,4,7)]) 
plot(df[,c(1,3,4,7)])
ggpairs(df[,c(1,3,4,7)])
#categorical variables
summary(df[,c(1,4,8:10)])

```

From the summary we can see the factor values, it seems that sex and
region are distributed equally and not much smokers compare to the non
smokers. age and number of children looks about right and there is
values in a range that makes sense. In addition, we see low correlation
(0.198) between the target variable and the other numeric explantory
variable bmi. We don't see any pattern in the relation between the two
variables. We see number of extreme values with high bmi and/or charges.

-   Determine if the response variable (charges) has an acceptably
    normal distribution.

```{r}
# Density plot to check the distribution
ggpubr::ggdensity(df$charges,  fill = "lightgray", add = "mean",  xlab = "charges variable density")
# Shapiro Test to asses that data on response variable is normaly distribution
# H0 = Data is normally distributed
# H1 = Data is not normally distributed
# alfa = 0.05
shapiro.test(df$charges)

```

As we can see, the density plot shows that data is not normally
distributed. To asses that, we can use one of many statistical tests
that check normality on data. In this case, we use Shapiro test.

The result of the Shapiro test shows that data in variable **charges**
is not normally distributed since *p-value* is less than the
significance level (0.05) so we reject the null hypothesis (data is
normally distributed) and we conclude that data is not normally
distributed (alternative hypothesis)

Let's try to apply the log transformation

```{r}
# Density plot to check the distribution
ggpubr::ggdensity(log(df$charges),  fill = "lightgray", add = "mean",  xlab = "charges variable density")

# Shapiro Test to asses that data on response variable is normaly distribution
# H0 = Data is normally distributed
# H1 = Data is not normally distributed
# alfa = 0.05
shapiro.test(log(df$charges))

```

The null hypothesis can be still rejected so data still not being not
normally distributed.

```{r}
par(mfrow=c(1,1))
acf(df$charges)
dwtest(df$charges~1)
```

Address tests to discard serial correlation: In the acf (auto
correlation function) we can see from the graph that the data is not
correlated where we have the blue threshold and all lines are within the
threshold, we do see that there is one or two lines that crosses the
threshold but just in a little bit so we leave it as it is without
random the order of the observations. In addition we address
Durbin-Watson test to check whether true autocorrelation is greater or
not than 0. We see p-value 0.5183, thus we don't reject the null
hypothesis and say that true autocorrelation is not greater than 0.

-   Preliminary exploratory analysis to describe the relationships
    observed has to be undertaken. [**Eliya**]

```{r}
library(DataExplorer)
create_report(df, y= "charges")
```

#THIS IS NOTE TO OURSELFS - there this libray which basically creates a
whole report of the explenatory data analysis, we can consider if we
want to put into as EDA is requested twice in the project statement,
once at data preparation and another on the tasks [Achraf: For me OK!]

-   Apart from the original factor variables, you can consider other
    categorical variables that can be defined from categorized numeric
    variables. [**Eliya**]

We have created a new variable called age_range where we divide the ages
into 4 groups according to the 4 quantiles. From the summary (and the
new column in the data set) we see 4 groups of ages and how many
observations were fit into each age group.
```{r}
df$age_range <- cut(df$age, breaks = quantile(df$age,probs = c(0,0.25,0.5,0.75,1)), include.lowest = T)
summary(df)
```

### Building the model

-   If you can improve linear relations or limit the effect of
    influential data, you must consider the suitable transformations for
    variables. [**Achraf**]

-   When building the model, you should study the presence of
    multicollinearity and try to reduce their impact on the model for
    easier interpretation. [**Achraf**]

-   You should build the model using a technique for selecting variables
    (removing no significant predictors and/or stepwise selection of the
    best models). [**Achraf**]

-   The validation of the model has to be done with graphs and / or
    suitable tests to verify model assumptions. [**Achraf**]

-   You must include the study of unusual and / or influential data.
    [**Achraf**]

#### First model

```{r}


par(mfrow=c(1,1))

plot(df$charges,df$bmi,pch=19)


#text(df$charges,df$bmi,label=row.names(df),col="darkgreen",adj=1.5)

m1<-lm(charges~bmi+age+children, data = df)
summary(m1)

lines(df$bmi,fitted(m1),col="red")

par(mfrow=c(2,2))
plot(m1)
par(mfrow=c(1,1))

```

Looking at the summary of the model, the RSquared is very low and there
is a lot of residual standard error.

If we study the residual error looking at the plots we can see that the
data is not following a normal distribution since there are deviations
of the line (Normal Q-Q plot). Also there are a lot of sparsity in the
variance (Scale-Location plot).

#### Asses multicollinearity

Maybe there is multicollinearity that is causing bad results

```{r}
car::vif(m1)
```

The vif values are low (less than 5) so there aren't problems of
multidisciplinary.

Let's try to do some transformations to the data.

#### Transformation

```{r}
library(MASS)

boxcox(charges~bmi+age+children, data = df)
```

The boxplots shows that the lambda values are close to 0 so a
logarithmic transformation to the target variable should help to improve
the results

```{r}

# (only for numerical variables)

boxTidwell(log(charges) ~ bmi + age +  I(children+0.5), data=df)

# poly(age,3) for adding ortogonal polynomials
```

-   TODO: check transformations for explanatory variables

```{r}

par(mfrow=c(1,1))
# apply logarithm to the charges variable
plot(log(df$charges),df$bmi,pch=19)

m2 <- lm(log(charges)~log(bmi)+ age+children, data = df)
lines(df$bmi, fitted(m2), color="red")
summary(m2)

par(mfrow=c(2,2))
plot(m2)
par(mfrow=c(1,1))

```

The model is still not performing very well. However if we check the
study of residuals we can see that it results in an improvement.

The normal Q-Q plot still have a deviation but is that big as the m1 and
if we check the Scale-Location of the standard residuals the variance is
better.

```{r}
avPlots(m2)
```

The partial regressions plots shows that all regresors have two big
clusters of data.

```{r}
AIC(m1,m2)
```

The AIC test shows that model 2 is performing much better than model 1
so we will continue with it.

#### Inlfuential data

Maybe, removing influential data the results can be improved.

-   Residual outliers

-   Influential values

```{r}

library(car)

influencePlot(m2)
# there are a lot of influential data

# Manually removing influential points
ll <- which(rownames(df) %in% c("1048", "848", "1318", "443")); ll
m3 <- lm(log(charges)~log(bmi)+age+children, data=df[-ll,])

summary(m3)
par(mfrow=c(2,2))
plot(m3)
par(mfrow=c(1,1))

influencePlot(m3)

# With cooks distance
cooksD <- cooks.distance(m2)
n <- nrow(df)
plot(cooksD, main = "Cooks Distance for Influential Obs")
abline(h = 4/n, lty = 2, col = "steelblue") # add cutoff line

# TODO: GET A BETTER THRESHOLD
influential_obs <- as.numeric(names(cooksD)[(cooksD > (4/n))])
influential_obs
length(influential_obs)
m4 <- lm(log(charges)~log(bmi)+age+children, data=df[-influential_obs,])
summary(m4)

par(mfrow=c(2,2))
plot(m4)
par(mfrow=c(1,1))

influencePlot(m4)
```

```{r}
#create scatterplot with outliers present
outliers_present <- ggplot(data = df, aes(x = log(bmi) + age + children, y = log(charges))) +
  geom_point() +
  geom_smooth(method = lm) +
#  ylim(0, 200) +
  ggtitle("Ifluential data Present")

#create scatterplot with outliers removed
outliers_removed <- ggplot(data = df[-influential_obs,], aes(x = log(bmi) + age + children, y = log(charges))) +
  geom_point() +
  geom_smooth(method = lm) +
#  ylim(0, 200) +
  ggtitle("Influential data Removed")

#plot both scatterplots side by side
gridExtra::grid.arrange(outliers_present, outliers_removed, ncol = 2)
```

#### Adding factors (very important)

-   Check that meaning of a factor could not be related to the numerical
    variables so one should be used.

-   AIC test to compare

```{r}

summary(df)
m5 <- lm(log(charges)~log(bmi)+age+children+sex+smoker+region+f.sex+f.smoker+f.region+age_range, data=df[-influential_obs,])
```

```{r}
summary(m5)
Anova(m5)

#remove age range
m6 <- lm(log(charges)~log(bmi)+age+children+sex+smoker+region+f.sex+f.smoker+f.region+age_range, data=df[-influential_obs,])

anova(m6, m5)
# models are not equivalent.
AIC(m6, m5)


m7 <- step( m5 )
summary(m7)

par( mfrow = c(2,2))
plot( m7, id.n=0 )
par( mfrow = c(1,1))

```

-   Redefining factors

```{r}
# New categorical variables can be extracted from the actual ones?
```

#### Adding interactions (very imporant)

-   Interactions between all factors (not a problem)

-   Double interactions (*ONLY*)

    -   Factor x factor

    -   Factor x numerical

-   plot **AllEffects** of partial regression to check what the model is
    doing with the interactions

```{r}
m8 <- lm(log(charges)~log(bmi)+age+children * (sex+smoker+region+f.sex+f.smoker+f.region+age_range), data=df[-influential_obs,])

summary(m8)

avPlots(m8)
#plot(allEffects(m8))
```

#### Validation of the model

-   Address residual outliers, influential data

-   It should be robust for exploratory variables (iterations)

-   Check if the models could be improved in some way

-   We should not expect a great coefficient of determination at the end
    (LIDIA)

    -   Maybe: Some critical explanatory is missing

```{r}

```
